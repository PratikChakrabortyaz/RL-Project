# ğŸ† Performance of MARL Algorithms in Cooperative and Asymmetric Soccer Environments

## ğŸ‘¥ Authors  
- **Pratik Chakraborty (220962350)**  
- **Priyanka Pathak (220962276)**  
- **Aryan Bhargava (220962129)**  

---

## ğŸ“˜ Project Overview  
This project presents a comparative study of **Multi-Agent Reinforcement Learning (MARL)** algorithms â€” **IPPO**, **MAPPO**, and **MADDPG** â€” implemented in the **Unity ML-Agents SoccerTwos** environment.  

The goal is to evaluate how different MARL algorithms perform under **cooperative** and **competitive** multi-agent settings, measuring their ability to learn coordinated strategies, maintain training stability, and achieve consistent performance.

---

## âš™ï¸ Environment Setup  

### ğŸ§© Requirements  

All implementations were executed on **Google Colab** using the following dependencies:

```bash
pip install torch torchvision torchaudio
pip install mlagents==0.30.0
pip install numpy matplotlib seaborn
pip install gym
```

---

## ğŸ•¹ï¸ Environment Details  

### Simulator  
- **Unity ML-Agents (SoccerTwos)**

### Observation Space  
- **Continuous** â€” includes positions, velocities, and ball state

### Action Space  
- **Discrete** â€” includes movement, kick, and direction

### Reward Structure  
- **Team-based cooperative rewards**  
  - Rewards for scoring and possession control

### Training Platform  
- **Google Colab GPU Runtime**

---

## ğŸ¤– Algorithms Implemented  

### 1. Independent Proximal Policy Optimization (IPPO)  
- Independent actorâ€“critic networks per agent  
- No shared parameters; decentralized training  
- Encourages independent exploration  
- Achieved steady policy improvement and balanced learning trajectories  

### 2. Multi-Agent Proximal Policy Optimization (MAPPO)  
- Uses a centralized critic with decentralized actors  
- Aggregates global state information for coordinated learning  
- Reduced policy oscillations and improved convergence stability  
- Demonstrated the most stable convergence among the three algorithms  

### 3. Multi-Agent Deep Deterministic Policy Gradient (MADDPG)  
- Actorâ€“critic structure with a shared critic  
- Optimized for continuous policy improvement  
- Agents learn coordinated offensive and defensive strategies  
- Exhibited rapid adaptability and high reward peaks  

---

## ğŸ“‚ Repository Structure  

```
MARL-SoccerTwos/
â”‚
â”œâ”€â”€ ğŸ“ scripts/
â”‚   â”œâ”€â”€ IPPO.ipynb        # Independent Proximal Policy Optimization implementation
â”‚   â”œâ”€â”€ MAPPO.ipynb       # Multi-Agent Proximal Policy Optimization implementation
â”‚   â”œâ”€â”€ MADDPG.ipynb      # Multi-Agent Deep Deterministic Policy Gradient implementation
â”‚
â”œâ”€â”€ ğŸ“„ RL_Project_Final_Report.pdf   # Project report with results and analysis
â”‚
â””â”€â”€ README.md                        # Project documentation (this file)
```

---

## ğŸš€ How to Use  

### ğŸ”¹ Step 1: Clone the Repository  

```bash
git clone https://github.com/your-username/MARL-SoccerTwos.git
cd MARL-SoccerTwos
```

### ğŸ”¹ Step 2: Open in Google Colab  

You can directly open the notebook files from the `scripts/` directory in **Google Colab**.

- Open `scripts/IPPO.ipynb` for Independent PPO  
- Open `scripts/MAPPO.ipynb` for Multi-Agent PPO  
- Open `scripts/MADDPG.ipynb` for Multi-Agent DDPG  

### ğŸ”¹ Step 3: Run the Training  

Each notebook contains:
- Environment setup and initialization  
- Actorâ€“critic network definitions  
- Training loop and policy updates  
- Visualization of rewards, losses, and win rates  

Execute the cells sequentially to train agents in the SoccerTwos environment.

---

## ğŸ“Š Results Summary  

| Algorithm | Architecture Type                   | Convergence | Coordination | Adaptability | Remarks                        |
|-----------|-------------------------------------|-------------|--------------|--------------|--------------------------------|
| IPPO      | Decentralized Actorâ€“Critic          | Moderate    | Medium       | Moderate     | Stable but slower learning     |
| MAPPO     | Centralized Critic, Decentralized Actors | Excellent   | High         | High         | Most stable and cooperative    |
| MADDPG    | Shared Critic (Continuous)          | High        | High         | Very High    | Most adaptive and responsive   |

---

## ğŸ’¡ Strengths  

- Fully functional multi-agent setup using Unity ML-Agents and PyTorch  
- Consistent training pipeline for all algorithms  
- Reproducible workflow on Google Colab  
- Comparative visualization of learning curves (rewards, policy loss, critic loss, win rates)  

---

## âš ï¸ Limitations  

- Limited training duration due to Colab GPU timeouts  
- Only the SoccerTwos environment implemented (Strikers vs Goalie pending)  
- Minimal hyperparameter tuning due to compute constraints  

---

## ğŸ”® Future Improvements  

- Extend training to larger episode counts for full convergence  
- Include asymmetric environments (e.g., Strikers vs Goalie)  
- Add advanced evaluation metrics such as entropy and coordination score  
- Visualize agent trajectories and heatmaps for interpretability
